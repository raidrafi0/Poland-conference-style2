%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a proceedings volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{svproc}
%
% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% to typeset URLs, URIs, and DOIs
\usepackage{array}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}
%\usepackage{subcaption}
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{color}
\usepackage{url}
\def\UrlFont{\rmfamily}
\newcommand{\tl}[1]{\textcolor{blue} {TL: #1 :TL} }
\renewcommand{\tt}[1]{\textcolor{red} {TT: #1 :TT} }
\begin{document}
	\mainmatter              % start of a contribution
	%
	\title{Road Tracking Using Deep Reinforcement Learning for Self-Driving Car Applications\thanks{This work is supported by EPSRC grants EP/P00430X/1 and EP/P015387/1.}}
	%
	\titlerunning{Road Tracking}  % abbreviated title (for running head)
	%                                     also used for the TOC unless
	%                                     \toctitle is used
	%
	\author{Raid Rafi Omar Al-Nima\inst{1,2} \and Tingting Han\inst{2} \and 
		Taolue Chen\inst{2}}
	%
	\authorrunning{Raid Al-Nima et al.} % abbreviated author list (for running head)
	%
	%%%% list of authors for the TOC (use if author list has to be modified)
	\tocauthor{Raid Rafi Omar Al-Nima, Tingting Han, and Taolue Chen}
	%
	\institute{
		%\email{raidrafi1@gmail.com}\\ 
		DCSIS, Birkbeck University of London	\and
		Technical Engineering College of Mosul, Northern Technical University, Iraq
		%\email{tingting@dcs.bbk.ac.uk,t.chen@dcs.bbk.ac.uk}
	}
	\maketitle              % typeset the title of the contribution
	\begin{abstract}
		Deep reinforcement learning has recently aroused wide attentions. It combines deep learning with reinforcement learning to solve difficult tasks. This paper proposes an efficient deep reinforcement learning network to solve the road tracking problem. We propose a neural network which collects input states from forward car facing views and produces suitable road tracking actions. The actions are derived from encoding the tracking directions and movements. We explored both value iteration and policy iteration based deep reinforcement learning algorithms. A comparison between the two and a comparison with existing networks are provided. Our results are very promising - the best driving accuracy achieved 93.94\%, the highest in the comparison. 
		% We would like to encourage you to list your keywords within
		% the abstract section using the \keywords{...} command.
		\keywords{Road Tracking, Deep Reinforcement Learning, Markov Decision Process}
	\end{abstract}
	%----------------------------------------------------------------------------------------------------
	\section{Introduction}
	Road tracking is one of the most challenging tasks emerged from key applications such as autonomous driving. It aims to automatically guide a car through the correct track without crashing other cars or objects.  
	%In a nutshell, it combines reinforcement learning (RL) and deep learning: 
	Reinforcement learning (RL). in a nutshell, is about an agent interacting with the environment, learning an optimal policy, by trial and error, for sequential decision making problems.
	%
	%in a wide range of fields in natural sciences, social sciences, and engineering (Sutton and Barto, 1998; 2018; Bertsekas and Tsitsiklis, 1996; Bertsekas, 2012; Szepesvari Â´ , 2010; Powell, 2011).
	%While the integration of reinforcement learning and neural networks has a long history, % (Sutton and Barto, 2018; Bertsekas and Tsitsiklis, 1996; Schmidhuber, 2015). With recent exciting achievements of
	%deep learning (LeCun et al., 2015; Goodfellow et al., 2016), benefiting from big data, powerful computation, new algorithmic techniques, mature software packages and architectures, and strong financial support, we have been witnessing the 
	%there is renaissance of reinforcement learning (Krakovsky, 2016), especially, 
	The combination of deep neural networks and reinforcement learning gives rise to deep reinforcement learning (deep RL). We hypothesise that deep reinforcement learning %\cite{} is considered as 
	can be one of the most promising %proficient 
	methods %that can be applied    
	to address 
	the road tracking challenge, and this paper %is concerned with 
	reports our approaches and results to apply DRL to accomplish some typical tasks in road tracking. 
	%road tracking. 
	%which usually %model the process as
	%takes a Markov Decision Process (MDP) as an explicit or implicit model. %where the deep reinforcement learning starts from a state, pick the best action by optimising considered rewards and produces a new state. The new state is then utilized as a current state, and the process continues.  
	%Different tracking tasks have been considered in the literature:
	
	One of the primary tasks in road tracking is to track objects \cite{Grigore2000Reinforcement,Cohen2010Reinforcement,Liu2004Reinforcement,Supancic2017Tracking}. %Other work considered different 
	Other tracking tasks include tracking and controlling velocity \cite{Jinlin2009Neurofuzzy}, tracking periodic gene repressilator \cite{Sootla2013On}, tracking single and double pendulum \cite{Hall2011Reinforcement}, and tracking maximum powers for wind speed conversion systems \cite{Wei2015Reinforcement}. 
	%More traditional approaches to these issues are discussed in the related work. 
	Applying DRL techniques to road tracking has been reported, but has not been explored thoroughly.  Perot \emph{et al.} investigated tracking roads of a driving car. A deep reinforcement learning was used 
	\cite{Perot2017End}. The main problem in this work is that the driving car oscillates around the main road track. This is due to the selected reward in this study, where it utilized the oriented angle of the road with the car speed. %In 2017 and 2018, 
	Yun \textit{et al.} \cite{Yun2017Action,Yun2018Action} proposed an action-decision network (ADNet) to track objects, %. The ADNet is a deep reinforcement learning network and it was applied in a quite complicated way, 
	where a pre-trained Convolutional Neural Network (CNN) was firstly employed then the reinforcement learning was utilized. 
	%\begin{itemize}
	%	\item \underline{Tracking Object:} In 2000, Grigore and Grigore proposed a tracking system controller by using a recurrent neural network with the reinforcement learning \cite{Grigore2000Reinforcement}. In 2010, Cohen and Pavlovic suggested an effective and vigorous real-time tracker. The tracker utilized the reinforcement learning and it was applied to tracking personal faces \cite{Cohen2010Reinforcement}. In 2004, Liu and Su proposed an object tracking method by exploiting the reinforcement learning. The reinforcement learning was employed here to determine the features of the tracking object \cite{Liu2004Reinforcement}. In 2017, Supan\v{c}i\v{c} and Ramanan constructed a learning policy for tracking objects. The reinforcement learning was applied to video streams to provide on-line decision \cite{Supancic2017Tracking}. 
	%	\item \underline{Different Tracking Issues:} In 2009, Jinlin \textit{et al.} presented a velocity tracking control approach. The authors utilized the neuro-fuzzy technique with the Q-learning in their work \cite{Jinlin2009Neurofuzzy}. In 2011, Hall \textit{et al.} illustrated a single and double pendulum tracking model. A traditional control, Bayesian computations and reinforcement learning were employed in this publication \cite{Hall2011Reinforcement}. In 2013, Sootla \textit{et al.} described a tracking periodic gene repressilator. The fitted Q iteration algorithm was applied to one dimensional signals in this study \cite{Sootla2013On}. In 2015, Wei \textit{et al.} explained tracking a maximum power for wind speed conversion systems. In this paper, a model-free Q-learning method was exploited \cite{Wei2015Reinforcement}. 
	%	\item \underline{Deep Leaning Tracking:} In 2017, Perot \textit{et al.} investigated tracking roads of a driving car. A deep reinforcement learning was used \cite{Perot2017End}. The main problem in this work is that the driving car oscillates around the main road track. This is due to the selected reward in this study, where it utilized the oriented angle of the road with the car speed. In 2017 and 2018, Yun \textit{et al.} approached an action-decision network (ADNet) to track objects. The ADNet is a deep reinforcement learning network and it was applied in a quite complicated way, where a pre-trained Convolutional Neural Network (CNN) was firstly employed then the reinforcement learning was utilized \cite{Yun2017Action}\cite{Yun2018Action}. 
	%\end{itemize}
	%It can be seen from the literature that the majority of tracking studies were focused on tracking object problems as in \cite{Grigore2000Reinforcement}\cite{Cohen2010Reinforcement}\cite{Liu2004Reinforcement}\cite{Supancic2017Tracking}. Other work considered different tracking tasks such as: tracking and controlling velocity \cite{Jinlin2009Neurofuzzy}, tracking periodic gene repressilator \cite{Sootla2013On}, tracking single and double pendulum \cite{Hall2011Reinforcement}, tracking maximum powers for wind speed conversion systems \cite{Wei2015Reinforcement}. Only several studies were concentrated on utilizing the deep learning in order to address tracking issues such as \cite{Yun2017Action,Yun2018Action}. 
	Our work contributes to this area by suggesting an effective road tracking procedure and employing the MDP based on the deep reinforcement learning, where states are used as inputs; rewards are utilized to evaluate the tracking policy and actions are predicted to provide new states. Effective coding for various road tracking possibilities of actions has been considered such as turning to the left or right and recognizing crossing object(s). 
	%This study can be exploited for driving car applications such as driverless (or automatic driving). 
	%After the introduction, this paper is organized as follows: Section 2 states the methodology with the theoretical concepts, Section 3 discusses the results and Section 4 concludes the paper.
	\paragraph{Related work.}  %\emph{Tracking Object:} In 2000, 
	For tracking objects, Grigore and Grigore \cite{Grigore2000Reinforcement} proposed a tracking system controller by using a recurrent neural network with the reinforcement learning. %In 2010, 
	Cohen and Pavlovic \cite{Cohen2010Reinforcement} suggested an effective and vigorous real-time tracker, which utilized RL and was applied to tracking personal faces. In 2004, Liu and Su proposed an object tracking method by exploiting the reinforcement learning. The reinforcement learning was employed here to determine the features of the tracking object \cite{Liu2004Reinforcement}. In 2017, Supan\v{c}i\v{c} and Ramanan constructed a learning policy for tracking objects. The reinforcement learning was applied to video streams to provide on-line decision \cite{Supancic2017Tracking}. 
	
	%\emph{Different Tracking Issues:} In 2009, Jinlin \textit{et al.} presented a velocity tracking control approach. The authors utilized the neuro-fuzzy technique with the Q-learning in their work \cite{Jinlin2009Neurofuzzy}. In 2011, Hall \textit{et al.} illustrated a single and double pendulum tracking model. A traditional control, Bayesian computations and reinforcement learning were employed in this publication \cite{Hall2011Reinforcement}. In 2013, Sootla \textit{et al.} described a tracking periodic gene repressilator. The fitted Q iteration algorithm was applied to one dimensional signals in this study \cite{Sootla2013On}. In 2015, Wei \textit{et al.} explained tracking a maximum power for wind speed conversion systems. In this paper, a model-free Q-learning method was exploited \cite{Wei2015Reinforcement}. 
	%	\emph{Deep Leaning Tracking:} In 2017, Perot \textit{et al.} investigated tracking roads of a driving car. A deep reinforcement learning was used \cite{Perot2017End}. The main problem in this work is that the driving car oscillates around the main road track. This is due to the selected reward in this study, where it utilized the oriented angle of the road with the car speed. In 2017 and 2018, Yun \textit{et al.} approached an action-decision network (ADNet) to track objects. The ADNet is a deep reinforcement learning network and it was applied in a quite complicated way, where a pre-trained Convolutional Neural Network (CNN) was firstly employed then the reinforcement learning was utilized \cite{Yun2017Action}\cite{Yun2018Action}. 
	
	%----------------------------------------------------------------------------------------------------------------
	\section{Modelling}
	In this section, we address various essential issues to model the road tracking, such as how to identify road crossing object(s), how to encode different tracking directions, and how to design the deep reinforcement learning network to determine the next action, etc. 
	\paragraph{The database.} Our research is based on the SYNTHIA-SEQS-05 database \cite{Ros2016TheSYNTHIA}. Video sequences were recorded and saved, from which one extracted front, rear, left and right view road images around the car for both right and left steering cars. Each view covers a range of angle up to 100 degrees. As a result, a large number of simulated image frames were provided, each has a resolution of $760 \times 1,280 \times 3$ pixel. 
	
	The images were taken in four different environments: (1) clear environment in spring, (2) fog, (3) rain and (4) heavy-rain environment. In our study, we look at \emph{forward} view road frames for a \emph{right} steering car in all \emph{four} environments. 
	Examples from the four databases are given in Table~\ref{Table:Environments_Examples}.	In our study, we look at \emph{forward} view road frames for a \emph{right} steering car in all \emph{four} environments. 
	
	
	\begin{table*}[!ht]
		\centering
		\caption{Examples of the four employed environments}
		\label{Table:Environments_Examples}
		\begin{tabular}{|C{2.5cm}|C{8.5cm}|}
			\hline
			\textbf{Environment} & \textbf{Examples} \\ \hline
			Spring & \begin{minipage}{.9\textwidth}\includegraphics[scale=.5,trim=2cm 24.5cm 2cm 2.5cm,clip]{examples.pdf}\end{minipage} \\ \hline
			%			&&\\ \hline
			Fog	& \begin{minipage}{.9\textwidth}\includegraphics[scale=.5,trim=2cm 20.5cm 2cm 6.5cm,clip]{examples.pdf}\end{minipage} \\ \hline
			%			&&\\ \hline
			Rain &  \begin{minipage}{.9\textwidth}\includegraphics[scale=.5,trim=2cm 16.5cm 2cm 10.5cm,clip]{examples.pdf}\end{minipage} \\ \hline
			%			&&\\ \hline
			Heavy-rain & \begin{minipage}{.9\textwidth}\includegraphics[scale=.5,trim=2cm 12.5cm 2cm 14.3cm,clip]{examples.pdf}\end{minipage} \\ \hline
			%			&&\\ \hline
		\end{tabular}
	\end{table*}
	
	
	\begin{figure*}[!ht]
		\centering
		\includegraphics[scale=.7,trim=3cm 22.5cm 7.1cm 2cm,clip]{segmentation_regions2.pdf}
		\vspace{-4ex}\caption{The suggested front view road tracking zones, lines and anchor point}
		\label{Fig:segmentation_regions1}\vspace{-0.5cm}
	\end{figure*}
	\subsection{Road tracking}
	To differentiate the main objects on the road, the databases provide specific colours for certain objects in the images, e.g., sky is grey, buildings are brown, roads are purple, side walks are blue and road markings are green. Overall, the purple and green colours refer to the allowed driving regions. %The coloured regions have been provided by \cite{Ros2016TheSYNTHIA}.	
	
	Based on the images, we further divide each view (image) into a safety zone and a danger zone. If objects are spotted in the safety zone the car can keep moving, as the zone is further away, allowing ample time to stop or slow down the car if necessary. The danger zone is the area that a car must stop if any objects are recognised. Such zones can be found in Fig.\,\ref{Fig:segmentation_regions1}. 
	
	To determine a safety zone or a danger zone, a virtual triangle and trapezoid are drawn. The base of the triangle and trapezoid moves in the road direction, and the crossing lines slope toward the tracking direction. The height of the triangle (and the trapezoid) is two thirds of the image. The top one third of the trapezoid is the safety zone and the bottom two thirds is the danger zone. The shared point of the triangle and the trapezoid is called an \emph{anchor point}. It denotes the road tracking direction by following the track centre. 
	
	By drawing the virtual triangle in the trapezoid area we can divide the region into left, right and centre region, with which the direction of any crossing objects can be specified. The car can, for instance, avoid an object crossing from the left side by moving to the right, if the space is empty there. If objects are identified from both sides in the danger zone, then the car has to stop. 
	 
	\subsection{Actions codes} \label{sec:action}
	At any point, a car can take one of the following actions: drive straight on, turn left or right, reverse and stop. In our work, we use a five-digit binary number to encode each action, see the table below. %\ref{Table:Signs_codes}. The stop action is applied when the danger zone recognising object(s) in the way from both sides. 
	%\begin{table}[!h]
	%	\centering
	%	\caption{The road tracking actions with their suggested codes and descriptions}
	%	\label{Table:Signs_codes}
	\vspace{-0.5cm}
	\begin{center}	\begin{tabular}{|C{2.5cm}|C{1.3cm}|C{2.5cm}|C{5.5cm}|}
			\hline
			\textbf{Action sign} & \textbf{Binary code} & \textbf{Equivalent decimal code} & \textbf{Description} \\ \hline
			Straightforward	& 01110	& 14 & follow the straight track\\ \hline
			Turn left	& 11100 & 28 & follow the track to the left \\ \hline
			Turn right	& 00111 & 7 & follow the track to the right \\ \hline
			Stop& 00000 & 0 & object(s) identified from both sides\\ \hline
			Backwards& 11111 & -31 & reverse \\ \hline
		\end{tabular}
		%\end{table}
	\end{center}
	When a crossing object is detected, it will change the code from that side with a ``0". For instance, a car is moving forward with action 01110, then an object appears in the danger zone from left, the action code becomes 00111, indicating that the car turns right to avoid the object. \tt{You didn't say when to turn 0 to 1. For instance why 01110 becomes 00111? It is only stated when 1 should be turned to 0 (with object appearing in the danger zone).}If another object appears from the right, the code will be 00110 and the car will have to stop. A car has to stop as long as there are no three consecutive 1's. We model all such cases as 00000 to reduce the number of coding values. Fig. 2 shows various coding cases. The cases in the red dashed box are all coded as 00000.
	The binary codes can be converted to desired equivalent decimal codes in the standard way. For instance, $(01110)_2=(14)_{10}$ and $(00111)_2=(7)_{10}$. The only exception here is the backward direction, where a negative sign (-) is added to refer to the reverse movement. It is worth mentioning that the reverse action will only be considered if the car is being out of the track. The decimal codes are used in the regression layer of the proposed deep reinforcement learning network as will be explained later. 
	%\vspace{-2ex}
	\begin{figure*}[!ht]
		\centering
		\includegraphics[scale=.33,trim=2cm 35cm 0cm 2cm,clip]{segmentation3.pdf}			
		\begin{scriptsize}\vspace{-4ex}\caption{Segmented images for road tracking: (a) straightforward, (b) turning left, (c) turning right, (d) reverse or backward, (e-g) stopping action because of a single crossing object, (h-j) stopping action because of two crossing objects and (k) stopping action because of three crossing objects}\end{scriptsize}
		%\caption{Segmented images for road tracking: (a) straightforward, (b) turning left, (c) turning right, (d) reverse or backward, (e-g) stopping action because of a single crossing object, (h-j) stopping action because of two crossing objects and (k) stopping action because of three crossing objects}
		\label{Fig:segmentation}
	\end{figure*}
	%\begin{table}[!h]
	%	\centering
	%	\caption{The road tracking actions with their suggested codes and descriptions}
	%	\label{Table:Signs_codes}
	%	\begin{tabular}{|C{3cm}|C{1.5cm}|C{3cm}|C{4cm}|}
	%		\hline
	%		\textbf{Action sign} & \textbf{Binary code} & \textbf{Equivalent decimal code} & \textbf{Description} \\ \hline
	%		\begin{minipage}{.075\textwidth}\includegraphics[scale=.5,trim=9.1cm 18.5cm 9.5cm 8cm,clip]{signs.pdf}\end{minipage}	& 0 0 0 0 0 & 0 & Stop (stop action because of crossing object(s)) \\ \hline
	%		\begin{minipage}{.075\textwidth}\includegraphics[scale=.5,trim=9.1cm 16cm 9.5cm 10.5cm,clip]{signs.pdf}\end{minipage}	& 0 1 1 1 0	& 14 & Move straight forward (follow the track to the forward direction) \\ \hline
	%		\begin{minipage}{.075\textwidth}\includegraphics[scale=.5,trim=9.1cm 13.5cm 9.5cm 13cm,clip]{signs.pdf}\end{minipage}		& 1 1 1 0 0 & 28 & Move to the left (follow the track to the left direction) \\ \hline
	%		\begin{minipage}{.075\textwidth}\includegraphics[scale=.5,trim=9.1cm 10.75cm 9.5cm 15.75cm,clip]{signs.pdf}\end{minipage}	& 0 0 1 1 1 & 7 & Move to the right (follow the track to the right direction) \\ \hline
	%		\begin{minipage}{.075\textwidth}\includegraphics[scale=.5,trim=9.1cm 8.5cm 9.5cm 18cm,clip]{signs.pdf}\end{minipage} & 1 1 1 1 1 & -31 & Move backward (reverse moving to the back) \\ \hline
	%	\end{tabular}
	%\end{table}
	%-----------------------------------------------------------------------------------------
%	\subsection{Policy search} 
%	%Principally, there are two essential types of reinforcement learning methods - policy iteration and value iteration. The first type refers to methods that consider searching for the optimal policy $\pi^*$. The second type refers to methods that consider investigating the optimal value-state function $V^*(s)$ \cite{Arulkumaran2017Deep}. 
%	The suggested DRL-RT network is based on the policy search. %According to the MDP concept, the 
%	DRL-RT collects input images as current states $S_t$ (or current views) and gets advantages from rewards $R$ to generate actions $A$. The actions then predict new states $S_{t+1}$ by following the track of the road. The process will be repeated in a multi-episodic manner.
%	
%	 %Consequently, the new states can be transitioned again in the inputs as current states. 
%	%The essential equation of the policy search is demonstrated as:
%	%\begin{equation}
%	%\pi^*=\underset{\pi}{argmax}~\mathbb{E} [R\mid\pi]
%	%\label{Eq:MDP}
%	%\end{equation}
%	%where $\pi$ is the policy and $\mathbb{E}$ is the expected return value \cite{arulkumaran2017brief}. 
%	In this work, the reward $R$ takes a simple form, i.e., the correct tracking is considered as (+1), whereas, the reward $R$ of the incorrect tracking is considered as (-1). Measuring the successful process of the road tracking will be based on obtaining as many positive rewards as possible. 
	
	\subsection{Proposed DRL-RT} 
	%An applicable deep reinforcement network named the DRL-RT is established for road tracking. It 
	In this section, we describe the neural network (NN) used in the Deep Reinforcement Learning framework for Road Tracking (DRL-RT), which consists of eight layers: two convention layers, two Rectified Linear Unit (ReLU) layers, a pooling layer, a fully connected layer, a regression layer and a classification layer. Fig.~\ref{Fig:Deep_Reinf_Net} depicts the proposed DRL-RT network.
\begin{figure*}[!ht]
	\vspace{-6ex}
	\centering
	\scalebox{0.8}{\includegraphics[scale=.5,trim=2cm 5.5cm 2cm 5.5cm,clip]{Deep_Reinf_Net1.pdf}}
	
	\vspace{-4ex}
	
	\begin{scriptsize}	\caption{The main design of the proposed DRL-RT. It consists of two conventional layers, two ReLU layers, a pooling layer, a fully connected layer, a regression layer and a classification layer\label{Fig:Deep_Reinf_Net}}\end{scriptsize}
	\vspace{-4ex}
\end{figure*}

The input of the NN is an image of a car facing view, and it is considered as the current state. The dimension of the input image is reduced to $254 \times 427 \times 3$ pixels to speed up the training. The first layer is a convolution layer which consists of 5 filters, each of which has a filter size of $10 \times 10$ pixels. This layer is to extract the main features of the input image. A ReLU layer is used next and it removes the negative values and maintain the positive values of the previous layer. The 3rd layer is also a convolution layer, consisting of 5 filters each of which has a filter size of $5 \times 5$ pixels. It extracts more features from the input images. A ReLU layer is employed again in the 4th layer. This layer rectifies the negative values. It has empirically been established that using two convolution layers with two ReLU layers can well analyse the information before being compressed by applying the next layer. Then, a pooling layer of a maximum type is applied as the 5th layerï¼ the filter size here is $3 \times 3$ pixels with a stride of 3 pixels. 

\tl{the sixth layer is to encode the policy?}
The 6th layer is a fully connected layer. It collects the outputs from the previous layer and produces a series of decimal code tracking values. \tt{The decimal code is generated here in the 6th layer and then in the 7th layer?}In the 7th regression layer, a series of directional road tracking codes are generated. \tl{what is successful unsuccessful code?} The successful code in this layer produce positive rewards, whereas, unsuccessful codes generate negative rewards. The network should propagate the information forward and backward to update the network weights, during the training stage till obtaining as many positive rewards as possible. Given the codes in the regression layer, it is the classification layer's task to generate a new action - one of the five as in Section \ref{sec:action}. 

We are to address a few technical details in the DRL-RT network. The underlying model of the network is a Markov Decision Process (MDP). 
\begin{itemize}
	\item \emph{States.} The states in the MDP are the views (images).  
	\item \emph{Rewards.} The reward $R$ takes a simple form, i.e., the correct tracking 	\tt{what do you mean by correct tracking? How to tell whether a tracking is correct? Comparing to the training set? Elaborate here.} is considered as (+1), whereas, the reward $R$ of the incorrect tracking is considered as (-1). Measuring the successful process of the road tracking will be based on obtaining as many positive rewards as possible. 

	
	\item \emph{Policy search.} The DRL-RT network is based on the policy search. %According to the MDP concept, the 
	DRL-RT collects input images as current states $S_t$ (or current views) and gets advantages from rewards $R$ to generate actions $A$. The actions then predict new states $S_{t+1}$ by following the track of the road. The process will be repeated in a multi-episodic manner.

\end{itemize}
 
	%--------------------------------------------------------------
	%\subsection{Theoretical concepts} 
	%\tl{is there anything which is not standard in this section!?}
	%The theoretical concepts of the main analysis layers (convolution, ReLU, pooling and fully connected) in the DRL-RT network were stated in \cite{omar2018deep}.
	%
	%In the first and third layers, the collected information will be converted to feature maps. The feature map is defined as a convoluted 2D image with a kernel of weights. The following general equation represents the operations in a convolution layer:
	%\begin{equation}
	%z_{u,v,c^{l}}= \text{\footnotesize $B_{c^{l}}+\sum_{i=-k_h^{l}}^{k_h^{l}}\sum_{j=-k_w^{l}}^{k_w^{l}}\sum_{c^{l-1}=1}^{C^{l-1}} W_{i+k_h^{l},j+k_w^{l},c^{l-1}}^{c^{l}} z_{u+i,v+j,c^{l-1}}$}
	%\label{eq:conv_layer}
	%\end{equation}
	%where $z_{u,v,c^{l}}$ is a convolution layer outcome, $(u,v)$ is the assigned pixel, $c^{l}$ is the channel number of the convolution layer,  $W_{i,j,c^{l-1}}^{c^{l}}$ is the components of the kernel weights,  $B_{c^{l}}$ is the channel bias of the convolution layer, $k_h^l$ and $k_w^l$ are respectively the height and width of the kernel weights of the convolution layer, $C$ is the number of channels and it is here equal to 3 as we are using three channels of coloured images, $l-1$ is the previous layer, and $l$ is the current layer (the convolution layer) \cite{simo2016learning}. 
	%% \marginpar{Does it mean that all the $k_w^l$ are the same for all layers?}. Answer is: no and I have mentioned previously that $l$ represents the current layer.
	%%Again, this layer analyses the input values and produces FTs' feature maps.
	%
	%A ReLU transfer function is applied in the second and fourth layers. This function can provide non-linear calculation to the DRL-RT. The ReLU function maintains the positive values and discards the negative values of a previous layer. Equation \eqref{eq:relu_layer} is exploited for the ReLU transfer function:
	%\begin{equation}
	%o_{u,v,c^{l}}=f(z_{u,v,c^{l}})=\max(0,z_{u,v,c^{l}})
	%\label{eq:relu_layer}
	%\end{equation}
	%where $o_{u,v,c^{l}}$ is a ReLU layer outcome and $\max$ is the maximum operation \cite{krizhevsky2012imagenet}. 
	%
	%A pooling layer is used in the fifth part of the DRL-RT. The pooling layer can reduce the sizes of the feature maps. It obtains the maximum values from the last ReLU layer. In general, the pooling layer can be applied according to the following equation:
	%\begin{equation}
	%q_{a^{l},b^{l},c}=\underset{0\leq a<p_h,0\leq b<p_w}{\max} o_{a^{l}\times p_h+a,~b^{l}\times p_w+b,~c}
	%\label{eq:pooling_layer}
	%\end{equation}
	%where $q_{a^{l},b^{l},c}$ is a pooling layer outcome, $0\leq a^{l} <p_h^{l}$, $p_h^{l}$ is the height of the resulting feature maps, $0\leq b^{l} <p_w^{l}$, $p_w^{l}$ is the width of the resulting feature maps, $0\leq c <C^{l}=C^{l-1}$, $p_h$ and $p_w$ are respectively the width and height of the feature map sub-areas that require pooling \cite{wu2017introduction}. 
	%
	%Subsequently, the fully connected layer is used to match between the designed number of subjects and the data of the pooling layer. Equation (\ref{eq:fully_connect}) demonstrates the fully connected layer processes:
	%\begin{equation}
	%g_{r}=\text{\footnotesize $\sum_{a=1}^{m_1^{l-1}} \sum_{b=1}^{m_2^{l-1}} \sum_{c=1}^{m_3^{l-1}} W_{a,b,c,r}^{l}(\textit{\textbf{Q}}_{c})_{a,b}~, ~~~~~~~\forall 1 \leq r \leq m^{l}$}
	%\label{eq:fully_connect}
	%\end{equation}
	%where $g_{r}$ is a fully connected layer outcome, $m_1^{l-1}$ and $m_2^{l-1}$ are the width and height of a feature map in the previous layer (the pooling layer) respectively, $m_3^{l-1}$ is the number of produced feature maps in the pooling layer, $W_{a,b,c,r}^{l}$ is the connection weights between the fully connected layer and the pooling layer, $\textit{\textbf{Q}}_{c}$ are the pooling layer outputs, and $m^{l}$ is the number of designed subjects \cite{stutz2014neural}.
	%
	%The computations of the regression layer in the suggested DRL-RT network are based on the Mean Squared Error (MSE). The main MSE equation is illustrated as:
	%\begin{equation}
	%MSE=\frac{1}{n} \sum_{r=1}^{n}(t_r-g_r)^2
	%\label{eq:fully_connect}
	%\end{equation}
	%where $n$ is the number of computed values and $t$ is the desired output values \cite{saugirouglu2009intelligent}. If the regression output values close to the desired code values, positive rewards are produced. Otherwise, negative rewards are generated. 
	%
	%Finally, the classification layer translates the regression information into actions by converting the obtained values into their assigned classes.
	%
	%\tl{cannot really see this; this is pretty standard RL stuffs}
	%The MDP has been applied to the DRL-RT by providing current road tracking as current states $S_t$ to the input layer, estimating tracking directions as actions $A$, considering correct and incorrect tracking actions as rewards $R$, and predicting next tracking movements as new states $S_{t+1}$. Then, the new state (or next tracking view) are transitioned as a current state to the DRL-RT input layer in order to produce a new state again.
	%==============================================================================================================
	\section{Results}
	\subsection{Implementation} 
%	Four databases from SYNTHIA \cite{Ros2016TheSYNTHIA} are used. The selected databases are constructed under different environment conditions: (1) spring, (2) fog, (3) rain and (4) heavy-rain. Moreover, their segmented images, which are provided by the same database, are found to be useful for manually determining the appropriate code of each track. Examples from the four databases are given in Table~\ref{Table:Environments_Examples}.	
%	\begin{table*}[!t]
%		\centering
%		\caption{Examples of the four employed environments}
%		\label{Table:Environments_Examples}
%		\begin{tabular}{|C{1.5cm}|c|C{8.5cm}|}
%			\hline
%			\textbf{Database no.} & \textbf{Environment} & \textbf{Examples} \\ \hline
%			(1)	& Spring & \begin{minipage}{.9\textwidth}\includegraphics[scale=.5,trim=2cm 24.5cm 2cm 2.5cm,clip]{examples.pdf}\end{minipage} \\ \hline
%			%			&&\\ \hline
%			(2) & Fog	& \begin{minipage}{.9\textwidth}\includegraphics[scale=.5,trim=2cm 20.5cm 2cm 6.5cm,clip]{examples.pdf}\end{minipage} \\ \hline
%			%			&&\\ \hline
%			(3)	& Rain &  \begin{minipage}{.9\textwidth}\includegraphics[scale=.5,trim=2cm 16.5cm 2cm 10.5cm,clip]{examples.pdf}\end{minipage} \\ \hline
%			%			&&\\ \hline
%			(4)	& Heavy- rain & \begin{minipage}{.9\textwidth}\includegraphics[scale=.5,trim=2cm 12.5cm 2cm 14.3cm,clip]{examples.pdf}\end{minipage} \\ \hline
%			%			&&\\ \hline
%		\end{tabular}
%	\end{table*}
	All implementations were performed on a PC with 8 GB RAM and Intel Core i5 processor (3.2 GHz). Only the microprocessor was used in this study to train and test the DRL-RT. We used 270, 284, 268 and 248 frames for the environments of spring, fog, rain and heavy-rain, respectively. The frames are equally divided between the training and testing stages (50\% each), where the odd-indexed frames are used in the training stage and the even-indexed frames are used in the testing stage.

	\subsection{Training stage} 
	The suggested DRL-RT network has been separately trained for each environment. The following parameters have been assigned for the trainings: Adaptive Moment Estimation (ADAM) optimizer \cite{kingma2014adam}, learning rate equal to 0.0003, gradient decay factor ($\beta_1$) equal to 0.9, squared gradient decay factor ($\beta_2$) equal to 0.99 and mini batch size equal to 128. 
	
	The training performance of the DRL-RT for the four databases is given in Fig.\,\ref{fig:training_curves}. This figure demonstrates the relationships between the Root Mean Square Error (RMSE) and the training iterations during the NN training stages. The RMSE values are usually exploited to demonstrate the differences between desired values and output values. These differences are usually reduced during the proceeding of training iterations. Clearly, the curves are successfully declined toward goals.
	
%		\begin{figure}[!ht]\tt{This figure was not complete.}
%		\centering
%		\includegraphics[scale=.5,trim=.5cm 10cm 4cm .5cm,clip]{training_curves.pdf}
%		\caption{The training performance of the DRL-RT for the: (1) spring environment, (2) fog environment, (3) rain environment and (4) heavy-rain environment}
%		\label{fig:training_curves}
%	\end{figure}
	\begin{figure}[!ht]
	\centering\vspace{-4ex}
	\includegraphics[scale=.5]{RMSE.png}
	\caption{The training RMSE vs \#iterations for the four environments}
	\label{fig:training_curves}
\end{figure}\vspace{-6ex}
	
	\subsection{Testing stage} 
	In the testing stage we use (driving) accuracy to indicate how well our model performed. \tt{Define driving accuracy.} Fig.\,\ref{fig:Main_Results} shows that the driving accuracy attained its highest value of 93.94\% by using the spring environment database. This is because that the DRL-RT has analysed very clear provided images.  The fog environment database obtained a high driving accuracy of 93.66\%. Here, the overall views are blurred but the road tracking can still be distinguished \tt{What is the road tracking in this case? Why can they still be distinguished in foggy environment?}, and the accuracy is only slightly lower than the spring views. The driving accuracy of the rain environment database achieved 89.55\% and this is due to the noise effects of rain drops on image views. Finally, the inferior driving accuracy of 84.68\% was recorded for the heavy-rain environment database as the amount of rain drops (or noise) is increased here.
	\begin{figure}[!ht]
		\centering
		\includegraphics[scale=.45,trim=3cm 10.3cm 2.7cm 10.3cm,clip]{Main_Results.pdf}
		\caption{The accuracy of the DRL-RT under different environments}
		\label{fig:Main_Results}
	\end{figure}
	
	%------------------------------------
	\subsection{Comparisons} 
	We have investigated and simulated various deep learning approaches to establish a fair comparisons between our DRL-RT method and other networks. Table \ref{Table:Comparisons} shows the accuracies of different deep learning networks by applying the database of SYNTHIA-SEQS-05-SPRING, some parameters were reasonably changed to allow acceptable comparisons. The reason of selecting this database here is that it has the clear environment, which is suitable to discard undesired effects and provide fair judgement.
	%In the case of comparisons, hard efforts were performed to investigate and simulate various deep learning approaches. 

\begin{table}
	\centering
	\caption{A comparison between the DRL-RT method and other suggested networks}
	\label{Table:Comparisons}
	\begin{tabular}{|C{5cm}|c|c|}
		\hline
		\textbf{Reference} & \textbf{Deep learning method} & \textbf{Accuracy} \\ \hline
		Karaduman and Eren \cite{Karaduman2017Deep} & CNN & 67.42\% \\ \hline
		Bojarski \textit{et al.} \cite{bojarski2016end} & CNN & 74.24\% \\ \hline
		George and Routray \cite{George2016Real} & CNN & 83.33\% \\ \hline
		Yun \textit{et al.} \cite{Yun2017Action,Yun2018Action} & ADNet & 83.33\% \\ \hline
		Mnih \textit{et al.} \cite{mnih2015human} & DQN & 88.64\% \\ \hline
		Proposed method & DRL-RT & 93.94\% \\ \hline
	\end{tabular}
\end{table}

From Table \ref{Table:Comparisons}, it can be seen that the suggested CNN in \cite{Karaduman2017Deep} obtained the inferior tracking accuracy of 67.42\%. This is due to the architecture of this network, where it constructed to classify directions of traffic signs. More specifically, a pooling layer was applied after each convolution layer and no ReLU layers were used. This caused compressing and wasting useful extracted features after each convolution layer. 

The CNN used in \cite{bojarski2016end} attained a low accuracy of 74.24\%. The main drawback of this network is that it considers steering angles to be tracked, where this increases the erroneous of obtaining precise outputs. 

The CNN used in \cite{George2016Real} achieved 83.33\%. This is also due to the architecture of this network, which was designed for classifying eye gaze directions. 

The ADNet used in \cite{Yun2017Action,Yun2018Action} attained the same accuracy of 83.33\%. The essential problem here is represented by the considered rewards, which were basically designed %after moving objects and not during the moving. 
for recognizing moved objects as the rewards are updated in the stop action. In addition, the Adnet architecture is not so appropriate for road tracking tasks. 


The Deep Q-Network (DQN) used in \cite{mnih2015human} and illustrated in \cite{arulkumaran2017brief} achieved reasonable accuracy of 88.64\%. This can be due to the DQN processes, where it combines between the deep network and the Q-learning. This network can be considered as the nearest method to our approach. 

Finally, our proposed method has shown superior performance by attaining the accuracy of 93.94\%. This can be due to the overall structure of our road tracking method including the network architecture, tracking policy and designed codes.

\section{Conclusion}
A deep reinforcement neural network DRL-RT is established for road tracking. This network is trained and tested to guide driving cars under different weather environments. Different tracking instances were coded to represent the appropriate road tracking. The MDP concept is used here, where the network accepted states and produced actions by taking advantages from rewards. The proposed deep reinforcement network is based on the policy search. This study were compared with other work and it reported superior performance. In addition, interesting results were benchmarked by the proposed approach. That is, the best performance of 93.94\% was recorded for a clear environment and reasonable performance were reported under unclear environments of fog, rain and heavy-rain as the obtained accuracies were respectively equal to 93.66\%, 89.55\% and 84.68\%. 
%\begin{thebibliography}{0}
%
%\bibitem{IEEEhowto:kopka}
%%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%
%\end{thebibliography}
%
% ---- Bibliography ----
%
\bibliographystyle{IEEEtr}
\bibliography{references14} 
\end{document}